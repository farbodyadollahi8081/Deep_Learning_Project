{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Learning Project\n",
        "#phase one\n",
        "\n",
        "Mohammad kalbasi:401211028\n",
        "\n",
        "Farbod Yadollahi: 98102595\n",
        "\n",
        "AmirMasoud Bagheri:400203616"
      ],
      "metadata": {
        "id": "H9DA-gyPolzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "first we install requirements and import liibraries"
      ],
      "metadata": {
        "id": "r5B_aq2ioo32"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOvIuPb2wYYl",
        "outputId": "52841819-4944-48c4-8055-0b57d218aa9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "pip install -U --no-cache-dir gdown --pre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-VL6MwvFwfA7"
      },
      "outputs": [],
      "source": [
        "# first importing libraries\n",
        "import base64\n",
        "import requests\n",
        "import numpy as np\n",
        "import gdown\n",
        "from zipfile import ZipFile\n",
        "import gzip as gzip\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "import re\n",
        "import cv2\n",
        "from skimage.transform import resize\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tqdm import tqdm\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import numpy  as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "\n",
        "import time\n",
        "import os\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyO2-xg-wjZ5",
        "outputId": "d15f4e77-b2c2-441c-9bb9-26768f37e8f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fh0esnpwsaE",
        "outputId": "b55f502d-004f-4119-f45e-cf352331ec08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/hukkelas/DSFD-Pytorch-Inference.git\n",
            "  Cloning https://github.com/hukkelas/DSFD-Pytorch-Inference.git to /tmp/pip-req-build-7t20pxwv\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/hukkelas/DSFD-Pytorch-Inference.git /tmp/pip-req-build-7t20pxwv\n",
            "  Resolved https://github.com/hukkelas/DSFD-Pytorch-Inference.git to commit fc1051d80936e3d7a6e34053bb5485c0a032e42e\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from face-detection==0.2.1) (1.21.6)\n",
            "Building wheels for collected packages: face-detection\n",
            "  Building wheel for face-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-detection: filename=face_detection-0.2.1-py3-none-any.whl size=29707 sha256=1c3096e0c349acc74e828b547c3a8a7e4bf42f5f687ee1cb2c1845fdf3a06759\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3mgylim3/wheels/57/d0/53/55657e0e64121cb64c10829c2f29bb3703afd0dcee55416e51\n",
            "Successfully built face-detection\n",
            "Installing collected packages: face-detection\n",
            "Successfully installed face-detection-0.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/hukkelas/DSFD-Pytorch-Inference.git\n",
        "import face_detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcltKA43wvJf",
        "outputId": "b8694afa-7b35-4f58-effb-6852747f9a38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MSCTD'...\n",
            "remote: Enumerating objects: 1217, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 1217 (delta 13), reused 7 (delta 3), pack-reused 1190\u001b[K\n",
            "Receiving objects: 100% (1217/1217), 102.24 MiB | 24.27 MiB/s, done.\n",
            "Resolving deltas: 100% (616/616), done.\n",
            "Updating files: 100% (934/934), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/XL2248/MSCTD.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K42c71hRwx94",
        "outputId": "61ba3069-b8d0-4830-c90d-c0e4ea6afb62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') # we use google drive to load files and save models there! so change path files based on your own data and run the code!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/gdrive/MyDrive/Phase_0_images"
      ],
      "metadata": {
        "id": "9S2JzJhOZ05n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we define a function to download the dataset from the drive"
      ],
      "metadata": {
        "id": "i8Ms-p_koyJr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pqioPnAsw09K"
      },
      "outputs": [],
      "source": [
        "def donwload_to_file(grdive_path,file_name,output_path):\n",
        "  \"\"\"\n",
        "  function for downloading zip files from google drive and save it to desired path on colab\n",
        "\n",
        "  inputs:\n",
        "    grdive_path: path of file we want to download (url should be in export=download format)\n",
        "    file_name: name of file we want to download\n",
        "    output_path:path we extract data into\n",
        "  outputs:\n",
        "    we dont have any output!\n",
        "\n",
        "  \"\"\"\n",
        "  gdown.download(grdive_path, file_name)\n",
        "  with ZipFile(file_name, 'r') as zipObj:\n",
        "    # Extract all the contents of zip file in current directory\n",
        "    zipObj.extractall(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we download the dataset"
      ],
      "metadata": {
        "id": "5bnBC4xso5qY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxzWffsxw4ez",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "outputId": "7250feec-4db6-47a5-d068-47b967d465da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access denied with the following error:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/u/0/uc?id=12HM8uVNjFg-HRZ15ADue4oLGFAYQwvTA&export=download \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadZipFile",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-151cce7e8a1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# for validation     https://drive.google.com/u/0/uc?id=12HM8uVNjFg-HRZ15ADue4oLGFAYQwvTA&export=download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdonwload_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://drive.google.com/u/0/uc?id=12HM8uVNjFg-HRZ15ADue4oLGFAYQwvTA&export=download'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dev.zip'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/content/gdrive/MyDrive/Phase_0_images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# for test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-ae5cddb2da84>\u001b[0m in \u001b[0;36mdonwload_to_file\u001b[0;34m(grdive_path, file_name, output_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \"\"\"\n\u001b[1;32m     13\u001b[0m   \u001b[0mgdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrdive_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzipObj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Extract all the contents of zip file in current directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mzipObj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1267\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
          ]
        }
      ],
      "source": [
        "# for train          \n",
        "donwload_to_file('https://drive.google.com/u/0/uc?id=1GAZgPpTUBSfhne-Tp0GDkvSHuq6EMMbj&export=download','train_ende.zip','/content/gdrive/MyDrive/Phase_0_images')\n",
        "# renaming train folder name (it's train_ende) to just train so it is more consistent with other folders names\n",
        "os.rename('/content/gdrive/MyDrive/Phase_0_images/train_ende','/content/gdrive/MyDrive/Phase_0_images/train')\n",
        "\n",
        "# for validation     https://drive.google.com/u/0/uc?id=12HM8uVNjFg-HRZ15ADue4oLGFAYQwvTA&export=download\n",
        "donwload_to_file('https://drive.google.com/u/0/uc?id=12HM8uVNjFg-HRZ15ADue4oLGFAYQwvTA&export=download','dev.zip','/content/gdrive/MyDrive/Phase_0_images')\n",
        "\n",
        "# for test\n",
        "donwload_to_file('https://drive.google.com/u/0/uc?id=1B9ZFmSTqfTMaqJ15nQDrRNLqBvo-B39W&export=download','test.zip','/content/gdrive/MyDrive/Phase_0_images')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "gdown.download('https://drive.google.com/drive/folders/1-5vqUdiViXZ8D0Pt6oMIm5mgS36J0v7s?usp=sharing')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "A7SpnGEREp0r",
        "outputId": "407464e5-7298-4b74-8e36-6fe514a76b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gdown/parse_url.py:35: UserWarning: You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: https://drive.google.com/uc?id=None\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/drive/folders/1-5vqUdiViXZ8D0Pt6oMIm5mgS36J0v7s?usp=sharing\n",
            "To: /content/1-5vqUdiViXZ8D0Pt6oMIm5mgS36J0v7s?usp=sharing\n",
            "959kB [00:00, 87.5MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1-5vqUdiViXZ8D0Pt6oMIm5mgS36J0v7s?usp=sharing'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#validation\n",
        "!gdown 'https://drive.google.com/drive/folders/1Cp7aaOPJGOUCgE2974ivtepsHxiLpuyU?usp=sharing'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ_7GuXlGM8t",
        "outputId": "2b3c48d6-2b81-48e7-ebed-9aaa9131f8bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gdown/parse_url.py:35: UserWarning: You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: https://drive.google.com/uc?id=None\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/drive/folders/1Cp7aaOPJGOUCgE2974ivtepsHxiLpuyU?usp=sharing\n",
            "To: /content/1Cp7aaOPJGOUCgE2974ivtepsHxiLpuyU?usp=sharing\n",
            "959kB [00:00, 82.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this is a dataset class to read the datasets' directory for images, texts and sentiments"
      ],
      "metadata": {
        "id": "HKTo7J84o8zg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SCv0qLAkw7nL"
      },
      "outputs": [],
      "source": [
        "class MSTCDDate(Dataset):\n",
        "  \"\"\"\n",
        "  data loader class for getting data but one image (and it corresponding dialogue and label at a time)\n",
        "  inputs:\n",
        "    image_path: folder path of image data\n",
        "    txt_path: folder path of txt data\n",
        "    language: language type(english,german,...)\n",
        "    mode: train,test,validation \n",
        "    image_format: jpg,png,...\n",
        "    sclae: upscaling or downscaling image\n",
        "    transform: transformation we want to apply on data\n",
        "    target_transform: transformation we want to apply on labels\n",
        "\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "   \n",
        "\n",
        "  def __init__(self,image_path,txt_path,language,mode,image_format,scale, transform=None, target_transform=None):\n",
        "     \n",
        "      text_loader = language + '_' + mode + '.txt'\n",
        "      sentiment_loader = 'sentiment_' + mode+'.txt'\n",
        "      index_file = 'image_index_' + mode + '.txt'\n",
        "      self.image_path = image_path\n",
        "      self.transform = transform\n",
        "      self.target_transform = target_transform\n",
        "      self.mode = mode\n",
        "      self.image_format = image_format\n",
        "      self.scale = scale\n",
        "      for file_names in os.scandir(txt_path):\n",
        "        name_file = os.path.basename(file_names)\n",
        "        if name_file == text_loader:\n",
        "          with open(file_names.path) as f:\n",
        "            self.text_file = [line.rstrip('\\n') for line in f]\n",
        "        if name_file == sentiment_loader:\n",
        "          with open(file_names.path) as f:\n",
        "            self.sentiment_file = [line.rstrip('\\n') for line in f]\n",
        "        if name_file == index_file:\n",
        "          with open(file_names.path) as f:\n",
        "            self.index_file = [line.rstrip('\\n') for line in f]\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.text_file)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      image_path_complete = self.image_path + '/' + self.mode + '/'\n",
        "      text_file_chosen = self.text_file[idx]\n",
        "      sentiment_chosen = int(self.sentiment_file[idx])\n",
        "      temp_image = cv2.imread(image_path_complete+str(idx)+ '.' + self.image_format)\n",
        "      n1,n2,n3 = np.shape(temp_image)\n",
        "      if self.scale != 1:\n",
        "        image = resize(temp_image,[int(n1/self.scale),int(n2/self.scale)], anti_aliasing=True)\n",
        "      else:\n",
        "        image = temp_image\n",
        "      if self.transform:\n",
        "          image = self.transform(image)\n",
        "      if self.target_transform:\n",
        "          label = self.target_transform(sentiment_chosen)\n",
        "      return  text_file_chosen,sentiment_chosen,image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we load the vgg image processing model from the previous part"
      ],
      "metadata": {
        "id": "mKpmRdE-pH94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_vgg_trained =torchvision.models.vgg16(pretrained=True)\n",
        "for param in model_vgg_trained.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "model_vgg_trained.classifier[6] = nn.Linear(4096, 3) # we have 3 output class\n",
        "model_vgg_trained = model_vgg_trained.to(device)\n",
        "if device == 'cpu':\n",
        "  model_vgg_trained.load_state_dict(torch.load('/content/gdrive/MyDrive/Phase_1_models/vgg_trained/gdrivemodel_vgg_trained_weights.pth',map_location=torch.device('cpu')))\n",
        "else:\n",
        "  model_vgg_trained.load_state_dict(torch.load('/content/gdrive/MyDrive/Phase_1_models/vgg_trained/gdrivemodel_vgg_trained_weights.pth'))\n",
        "model_vgg_trained.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981,
          "referenced_widgets": [
            "cbc304ededc24f948a096329873c93ca",
            "97e19304a38943b88285c3de64b79f1e",
            "5948eb42e4254cd3baeab93cfe033954",
            "1911737ed9934f6780fe07d35c1105e4",
            "d208d5e04ad84bf5bbe73eadd8191341",
            "26ac259411b94faeb938608902fd47cd",
            "18d1e32b481b46879788f946a251c271",
            "2f2afa822e954efd90c6f3a7f991937a",
            "1feeadf5a1b54c409eaa2d95fb64add9",
            "7a1247bec64a41ddbcd96a6ebe6fc3ef",
            "ca05efafcac044b78a24cd7e68862f93"
          ]
        },
        "id": "tD5KmNJkuInh",
        "outputId": "533f39e2-3674-4c76-a0cc-16d06e429a5d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/528M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cbc304ededc24f948a096329873c93ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_vgg_trained)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-om8NRRxRUgw",
        "outputId": "b30caa5b-0a0d-43f1-f27b-c3814010bfee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=3, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we load the resnet model for face analysis part"
      ],
      "metadata": {
        "id": "vA-3kE6YpOvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_resnet_trained= torchvision.models.resnet50(pretrained=True)\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = model_resnet_trained.fc.in_features\n",
        "model_resnet_trained.fc = nn.Linear(num_ftrs, 3) # we have 10 output class\n",
        "\n",
        "model_resnet_trained= model_resnet_trained.to(device)\n",
        "if device == 'cpu':\n",
        "  model_resnet_trained.load_state_dict(torch.load('/content/gdrive/MyDrive/Phase_1_models/resnet_trained',map_location=torch.device('cpu')))\n",
        "else:\n",
        "  model_resnet_trained.load_state_dict(torch.load('/content/gdrive/MyDrive/Phase_1_models/resnet_trained'))\n",
        "model_resnet_trained.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ad6b896fe1b24d98bc8e1c7531e46882",
            "849ebfe1e5844bf88d5e35123296126d",
            "885a6216816341f2b8d201161e526652",
            "985aca2907484a4faa6f9755b2a91d2a",
            "5bab55662b0e48d38652522e0fbdbead",
            "2fb15503dadd4dbfb9f6c20b2763f5d6",
            "d19ac633783641e98dee566bb32f2608",
            "d437e00e204f4bb188bf61284008de56",
            "ad5fefe0ff674b328f6fbbee0b9389dc",
            "9c03eaa59f7f4961a69980ec05c55787",
            "9e31e3432d4545188cc2c5723c8107c6"
          ]
        },
        "id": "E7YESk4h1_DL",
        "outputId": "ecdf150c-7da7-42e4-df31-6bc3e06e22ac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad6b896fe1b24d98bc8e1c7531e46882"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_resnet_trained)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dV5LN8nixR8",
        "outputId": "37d9a6c7-f59a-40a5-9e25-761397e0ad99"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this is the mobilenet module for face detection"
      ],
      "metadata": {
        "id": "X78FqmzFpWaP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "g1-wkWxbxAGN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "c304e8f066e0465891f956622e3106c4",
            "44e96b33dc1e491f9d23cc97daf193cc",
            "77015872652d45288d002e5d31712346",
            "6b7633308bdd421280f1897c57e8ff63",
            "33a2ea36dfe34b6b88070a121f49f123",
            "908b4374738944a2822d3c6cd24b2532",
            "8e961932ecba41aa8124f57ae6849999",
            "3c4bf9796c3f418fb93ff1e221ddf0af",
            "f4151b27d53a40ce9c1693c9e85ef2c3",
            "2f131bf76cef4c32932939ac86ede070",
            "396b2ce674d04da79041168be66461f5"
          ]
        },
        "outputId": "6549ea7d-6bf6-46af-fc25-5a13b2bda538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://folk.ntnu.no/haakohu/RetinaFace_mobilenet025.pth\" to /root/.cache/torch/hub/checkpoints/RetinaFace_mobilenet025.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/1.71M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c304e8f066e0465891f956622e3106c4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "detector_mobilenet = face_detection.build_detector(\"RetinaNetMobileNetV1\", confidence_threshold=.7, nms_iou_threshold=.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we read the dataset with our MSCTD dataset class"
      ],
      "metadata": {
        "id": "sW5dtXgkpc7X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7Z1-BU2xDq4"
      },
      "outputs": [],
      "source": [
        "data_train = MSTCDDate('/content/gdrive/MyDrive/Phase_0_images','/content/MSCTD/MSCTD_data/ende','english','train','jpg',1)\n",
        "text_file_chosen,sentiment_chosen,image = data_train[0] "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_val = MSTCDDate('/content/gdrive/MyDrive/Phase_0_images/dev','/content/MSCTD/MSCTD_data/ende','english','dev','jpg',1)"
      ],
      "metadata": {
        "id": "bA810QQgH3tn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = MSTCDDate('/content/gdrive/MyDrive/Phase_0_images/test','/content/MSCTD/MSCTD_data/ende','english','test','jpg',1)"
      ],
      "metadata": {
        "id": "WEk_x2BLIIpA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here are the transforms used for each previous parts"
      ],
      "metadata": {
        "id": "tyziIqc4pj4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformation_resnet= transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n",
        "transformation_vgg= transforms.Compose([transforms.ToTensor(),transforms.Resize((50,100)),transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])"
      ],
      "metadata": {
        "id": "5GjV4b6BIO3t"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we load the labels"
      ],
      "metadata": {
        "id": "bzmxGCuupp7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/MSCTD/MSCTD_data/ende/sentiment_train.txt') as f:\n",
        "  sentiment_train = [int(line.rstrip('\\n')) for line in f]\n",
        "with open('/content/MSCTD/MSCTD_data/ende/sentiment_dev.txt') as f:\n",
        "  sentiment_dev = [int(line.rstrip('\\n')) for line in f]\n",
        "with open('/content/MSCTD/MSCTD_data/ende/sentiment_test.txt') as f:\n",
        "  sentiment_test = [int(line.rstrip('\\n')) for line in f]"
      ],
      "metadata": {
        "id": "r_M7g_8cFZAq"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we iterate through the images in train dataset in batch mode. for each batch, we first find the output of the image processing model-vgg 16. then we detect faces by the use of mobilenet module and pass first 6 detected faces to our face analysis module. number of detected faces followed by 6 faces analysis outputs and image processing output are concatenated into a vector of $1+6 \\times 3 + 1 \\times 3 = 22$ elements for each data to be used for our combine model which will be defined later."
      ],
      "metadata": {
        "id": "_g7CD5w0ps2W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-2DcBvjxGVB",
        "outputId": "78ccf1ba-a4e9-41f2-ec70-f475b85e1324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "extracting faces... :   2%|         | 403/20240 [02:52<2:10:35,  2.53it/s]"
          ]
        }
      ],
      "source": [
        "sentiment_list = []\n",
        "inp=torch.zeros(len(data_train),7*3+1)\n",
        "counter = 0\n",
        "for i in tqdm(range(len(data_train)), total = len(data_train), desc=\"extracting faces... \"):\n",
        "  help=[]\n",
        "  text_file_chosen,sentiment_chosen,image = data_train[i] # loading file\n",
        "  imagep=image[:,:,[2,1,0]]\n",
        "  image1=torch.unsqueeze(transformation_vgg(imagep),0)\n",
        "  image1=image1.to(device)\n",
        "  with torch.no_grad():\n",
        "    image_out=model_vgg_trained(image1)\n",
        "  #image2=torch.unsqueeze(transformation_resnet(image),0)\n",
        "  #image2=image2.to(device)\n",
        "  face_all = detector_mobilenet.detect(image)\n",
        "  inp[i,0]=len(face_all)\n",
        "  c=0\n",
        "  for xmin,ymin,xmax,ymax,confidence in face_all:\n",
        "    if c>=6:\n",
        "      break;\n",
        "    if xmin < 0 :\n",
        "      xmin = 0\n",
        "    if ymin < 0:\n",
        "      ymin = 0\n",
        "    sentiment_list.append(sentiment_chosen)\n",
        "    temp = image[int(ymin): int(ymax),int(xmin):int(xmax),:]\n",
        "    temp = resize(temp,[int(100),int(75)], anti_aliasing=True)\n",
        "    temp = temp[:,:,[2,1,0]]\n",
        "    temp = torch.unsqueeze(transformation_resnet(temp),0)\n",
        "    temp = temp.float()\n",
        "    temp=temp.to(device)\n",
        "    with torch.no_grad():\n",
        "      face=model_resnet_trained(temp)\n",
        "    inp[i,c+1:c+4]=face\n",
        "    c+=1\n",
        "  inp[i,-4:-1]=image_out  \n",
        "  counter = counter + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we save the output vectors in drive"
      ],
      "metadata": {
        "id": "8TIDTKEUqv7I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ucnFCtKKQp8"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(\"/content/sample_data/inp\", \"wb\") as fp:   #Pickling\n",
        "  pickle.dump(inp, fp)\n",
        "with open(\"/content/sample_data/sentiment_list\", \"wb\") as fp:   #Pickling\n",
        "  pickle.dump(sentiment_list, fp)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we load those vectors from the drive"
      ],
      "metadata": {
        "id": "fhz8RVoGq3d6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"/content/gdrive/MyDrive/sentiment_list/sentiment_list\", \"rb\") as fp:   #Pickling\n",
        "  sentiment_list =  pickle.load(fp)\n",
        "with open(\"/content/gdrive/MyDrive/inp/inp\", \"rb\") as fp:   #Pickling\n",
        "  inp = pickle.load(fp)"
      ],
      "metadata": {
        "id": "JyrEu5vWUL9v"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we do the whole above things for validation and test data"
      ],
      "metadata": {
        "id": "tg0Rqwxfq8JU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for validation\n",
        "sentiment_list_v = []\n",
        "inp_v=torch.zeros(len(data_val),7*3+1)\n",
        "counter = 0\n",
        "for i in tqdm(range(len(data_val)), total = len(data_val), desc=\"extracting faces... \"):\n",
        "  text_file_chosen,sentiment_chosen,image = data_val[i] # loading file\n",
        "  imagep=image[:,:,[2,1,0]]\n",
        "  image1=torch.unsqueeze(transformation_vgg(imagep),0)\n",
        "  image1=image1.to(device)\n",
        "  with torch.no_grad():\n",
        "    image_out=model_vgg_trained(image1)\n",
        "  #image2=torch.unsqueeze(transformation_resnet(image),0)\n",
        "  #image2=image2.to(device)\n",
        "  face_all = detector_mobilenet.detect(image)\n",
        "  inp_v[i,0]=len(face_all)\n",
        "  c=0\n",
        "  for xmin,ymin,xmax,ymax,confidence in face_all:\n",
        "    if c>=6:\n",
        "      break;\n",
        "    if xmin < 0 :\n",
        "      xmin = 0\n",
        "    if ymin < 0:\n",
        "      ymin = 0\n",
        "    sentiment_list_v.append(sentiment_chosen)\n",
        "    temp = image[int(ymin): int(ymax),int(xmin):int(xmax),:]\n",
        "    temp = resize(temp,[int(100),int(75)], anti_aliasing=True)\n",
        "    temp = temp[:,:,[2,1,0]]\n",
        "    temp = torch.unsqueeze(transformation_resnet(temp),0)\n",
        "    temp = temp.float()\n",
        "    temp=temp.to(device)\n",
        "    with torch.no_grad():\n",
        "      face=model_resnet_trained(temp)\n",
        "    inp_v[i,c+1:c+4]=face\n",
        "    c+=1\n",
        "  inp_v[i,-4:-1]=image_out  \n",
        "  counter = counter + 1"
      ],
      "metadata": {
        "id": "M2aA03o2V3Fm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "270745ef-d41e-487b-e65a-4772dc128cec"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "extracting faces... : 100%|| 5063/5063 [1:09:47<00:00,  1.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"/content/gdrive/MyDrive/inp_v\", \"wb\") as fp:   #Pickling\n",
        "  pickle.dump(inp_v, fp)\n",
        "with open(\"/content/gdrive/MyDrive/sentiment_list_v\", \"wb\") as fp:   #Pickling\n",
        "  pickle.dump(sentiment_list_v, fp)\n"
      ],
      "metadata": {
        "id": "mQgaZ8M0Z5d-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/gdrive/MyDrive/sentiment_list_v\", \"rb\") as fp:   #Pickling\n",
        "  sentiment_list_v =  pickle.load(fp)\n",
        "with open(\"/content/gdrive/MyDrive/inp_v\", \"rb\") as fp:   #Pickling\n",
        "  inp_v = pickle.load(fp)"
      ],
      "metadata": {
        "id": "0snPF-rwZ9UU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for test\n",
        "sentiment_list_t = []\n",
        "inp_t=torch.zeros(len(data_test),7*3+1)\n",
        "counter = 0\n",
        "for i in tqdm(range(len(data_test)), total = len(data_test), desc=\"extracting faces... \"):\n",
        "  text_file_chosen,sentiment_chosen,image = data_test[i] # loading file\n",
        "  imagep=image[:,:,[2,1,0]]\n",
        "  image1=torch.unsqueeze(transformation_vgg(imagep),0)\n",
        "  image1=image1.to(device)\n",
        "  with torch.no_grad():\n",
        "    image_out=model_vgg_trained(image1)\n",
        "  #image2=torch.unsqueeze(transformation_resnet(image),0)\n",
        "  #image2=image2.to(device)\n",
        "  face_all = detector_mobilenet.detect(image)\n",
        "  inp_t[i,0]=len(face_all)\n",
        "  c=0\n",
        "  for xmin,ymin,xmax,ymax,confidence in face_all:\n",
        "    if c>=6:\n",
        "      break;\n",
        "    if xmin < 0 :\n",
        "      xmin = 0\n",
        "    if ymin < 0:\n",
        "      ymin = 0\n",
        "    sentiment_list_t.append(sentiment_chosen)\n",
        "    temp = image[int(ymin): int(ymax),int(xmin):int(xmax),:]\n",
        "    temp = resize(temp,[int(100),int(75)], anti_aliasing=True)\n",
        "    temp = temp[:,:,[2,1,0]]\n",
        "    temp = torch.unsqueeze(transformation_resnet(temp),0)\n",
        "    temp = temp.float()\n",
        "    temp=temp.to(device)\n",
        "    with torch.no_grad():\n",
        "      face=model_resnet_trained(temp)\n",
        "    inp_t[i,c+1:c+4]=face\n",
        "    c+=1\n",
        "  inp_t[i,-4:-1]=image_out  \n",
        "  counter = counter + 1"
      ],
      "metadata": {
        "id": "S78ORY--Z-8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0c8e17e-f805-44e2-f753-e885480fd579"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "extracting faces... : 100%|| 5067/5067 [41:30<00:00,  2.03it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"/content/gdrive/MyDrive/inp_t\", \"wb\") as fp:   #Pickling\n",
        "  pickle.dump(inp_t, fp)\n",
        "with open(\"/content/gdrive/MyDrive/sentiment_list_t\", \"wb\") as fp:   #Pickling\n",
        "  pickle.dump(sentiment_list_t, fp)\n"
      ],
      "metadata": {
        "id": "suAh1q8Ga0H2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/gdrive/MyDrive/sentiment_list_t\", \"rb\") as fp:   #Pickling\n",
        "  sentiment_list_t =  pickle.load(fp)\n",
        "with open(\"/content/gdrive/MyDrive/inp_t\", \"rb\") as fp:   #Pickling\n",
        "  inp_t = pickle.load(fp)"
      ],
      "metadata": {
        "id": "AanDCdf2a4zr"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we define a new dataset class for the saved vectors"
      ],
      "metadata": {
        "id": "UbMWR2vArHn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class comb_Dataset(Dataset):\n",
        "    def __init__(self,inp,labels):\n",
        "        self.inp = inp\n",
        "        self.lables = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lables)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        inp = self.inp[idx]\n",
        "        lables = self.lables[idx]\n",
        "        return inp,lables"
      ],
      "metadata": {
        "id": "lRB19c7Ra_e1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we load the new dataset"
      ],
      "metadata": {
        "id": "XLzTMmtUrMiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_dataset= comb_Dataset(inp=inp,labels=sentiment_train)\n",
        "train_loader= DataLoader(train_dataset,batch_size=batch_size,shuffle = True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ch1CdeE1cRkH"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMYYEBDmD5E5",
        "outputId": "13a55994-4324-41f6-c7a6-2d512fafe9a1"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inp.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u-C52hqEJWx",
        "outputId": "6a5b99eb-64dc-4c7b-a8b3-8538dbfc9d88"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20240, 22])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = comb_Dataset(inp=inp_v,  labels=sentiment_dev)\n",
        "val_loader=   DataLoader(val_dataset,batch_size=batch_size,shuffle = False)"
      ],
      "metadata": {
        "id": "lHGGrqp0PCpK"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(val_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcZB01YSLcyl",
        "outputId": "744013dc-dfc5-4ffd-9dd7-82ce3b031a78"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = comb_Dataset(inp=inp_t,  labels=sentiment_test)\n",
        "test_loader= DataLoader(test_dataset,batch_size=64,shuffle = False)"
      ],
      "metadata": {
        "id": "O28yaXbQPCes"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we define a simple MLP to classify the vectors into the associated sentiments"
      ],
      "metadata": {
        "id": "Jm_KR8d2rSOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.layers=nn.Sequential(\n",
        "        nn.Linear(3*(6+1)+1,64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64,16),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(16,32),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(32,8),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(8,64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64,16),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(16,32),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(32,8),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(8,64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64,16),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(16,32),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(32,8),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(8,3)\n",
        "        )\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "\n",
        "net = Net()"
      ],
      "metadata": {
        "id": "Py-AZmnAdCnc"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here is the training loop"
      ],
      "metadata": {
        "id": "ZaRYxIoQrn31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_manual(model, criterion, optimizer,scheduler,val_beark,data_loader_train,data_loader_val,len_train,len_val, num_epochs=25):\n",
        "\n",
        "  train_acc = []\n",
        "  train_loss = []\n",
        "  test_acc = []\n",
        "  test_loss = []\n",
        "  since = time.time()\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "  best_loss = 100000000000\n",
        "  counter_val_beark = 0\n",
        "  for epoch in range(num_epochs):\n",
        "    ### Training\n",
        "    model.train()\n",
        "    loss_train = 0\n",
        "    acc_train = 0\n",
        "    counter = 1\n",
        "    for batch,label in tqdm(data_loader_train, total=len(data_loader_train), desc=\"training... \"):\n",
        "      # 1. Forward pass\n",
        "      batch = batch.to(device)\n",
        "      label = label.to(device)\n",
        "      outputs = model(batch) # model outputs raw logits \n",
        "      _, preds = torch.max(outputs, 1)\n",
        "\n",
        "      # print(y_logits)\n",
        "      # 2. Calculate loss and accuracy\n",
        "      counter = counter + 1\n",
        "      loss = criterion(outputs, label)\n",
        "      acc_train += torch.sum(preds == label.data)\n",
        "      loss_train = loss_train + loss\n",
        "      # 3. Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 4. Loss backwards\n",
        "      loss.backward()\n",
        "\n",
        "      # 5. Optimizer step\n",
        "      optimizer.step()\n",
        "    loss_train = loss_train/20240*100\n",
        "    acc_train = acc_train/20240*100\n",
        "    train_acc.append(acc_train)\n",
        "    train_loss.append(loss_train)\n",
        "    scheduler.step()\n",
        "    ### Testing\n",
        "    loss_test = 0\n",
        "    acc_test = 0\n",
        "    counter = 1\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      for batch,label in tqdm(data_loader_val, total=len(data_loader_val), desc=\"Validating... \"):\n",
        "      # 1. Forward pass\n",
        "        batch = batch.to(device)\n",
        "        label = label.to(device)\n",
        "        output = model(batch) # model outputs raw logits \n",
        "        _, preds = torch.max(output, 1)\n",
        "        loss_test= criterion(output, label.data)\n",
        "        acc_test += torch.sum(preds == label.data)\n",
        "      loss_test = loss_test/5063*100\n",
        "      acc_test = acc_test/5063*100\n",
        "      test_loss.append(loss_test)\n",
        "      test_acc.append(acc_test)\n",
        "    if acc_test > best_acc:\n",
        "                best_acc = acc_test\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    if loss_test <= best_loss:\n",
        "                best_loss = loss_test\n",
        "                counter_val_beark = 0\n",
        "    if loss_test > best_loss:\n",
        "                counter_val_beark = counter_val_beark + 1\n",
        "                if (counter_val_beark > val_beark):\n",
        "                  print(f\"early stopping happend!\")\n",
        "                  break;\n",
        "\n",
        "    # Print out what's happening\n",
        "    if epoch % 1 == 0:\n",
        "      print(f\"Epoch: {epoch} | Loss: {loss_train:.5f}, Acc: {acc_train:.2f}% | Test Loss: {loss_test:.5f}, Test Acc: {acc_test:.2f}%\") \n",
        "  time_elapsed = time.time() - since\n",
        "  print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "  print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "  # load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model,best_loss,best_acc"
      ],
      "metadata": {
        "id": "RdblgHpRdODA"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we train the model"
      ],
      "metadata": {
        "id": "Q31HACDsrsX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as\n",
        "# opposed to before.\n",
        "optimizer= optim.Adam(net.parameters(), lr=0.001)\n",
        "# Decay LR by a factor of 0.5 every 20 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "net = net.to(device)\n",
        "model_comb_trained,best_loss,best_acc = train_model_manual(net, criterion, optimizer,exp_lr_scheduler,20,train_loader,val_loader,len(train_loader),len(val_loader),num_epochs=100)"
      ],
      "metadata": {
        "id": "QSjMO6mVgA1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aee87683-f66d-4e0c-9890-ecc0e5db516d"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 284.58it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1542.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 1.71323, Acc: 37.43% | Test Loss: 0.02111, Test Acc: 36.30%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 282.81it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1502.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | Loss: 1.70210, Acc: 39.45% | Test Loss: 0.02100, Test Acc: 37.35%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 273.73it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1477.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 | Loss: 1.69942, Acc: 39.91% | Test Loss: 0.02079, Test Acc: 37.43%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 269.95it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1468.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 | Loss: 1.69865, Acc: 39.89% | Test Loss: 0.02083, Test Acc: 37.82%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 277.76it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1521.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 | Loss: 1.69768, Acc: 40.00% | Test Loss: 0.02099, Test Acc: 37.72%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 284.55it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1479.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5 | Loss: 1.69750, Acc: 39.77% | Test Loss: 0.02076, Test Acc: 37.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 279.76it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1543.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6 | Loss: 1.69744, Acc: 40.18% | Test Loss: 0.02097, Test Acc: 37.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 284.41it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1559.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7 | Loss: 1.69643, Acc: 40.25% | Test Loss: 0.02067, Test Acc: 37.21%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 280.28it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1386.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8 | Loss: 1.69683, Acc: 40.30% | Test Loss: 0.02074, Test Acc: 37.09%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 280.59it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1496.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9 | Loss: 1.69641, Acc: 40.37% | Test Loss: 0.02099, Test Acc: 37.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 289.41it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1530.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 | Loss: 1.69437, Acc: 40.67% | Test Loss: 0.02095, Test Acc: 37.63%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 284.79it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1605.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11 | Loss: 1.69425, Acc: 40.71% | Test Loss: 0.02093, Test Acc: 37.59%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 291.12it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1571.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 12 | Loss: 1.69363, Acc: 40.61% | Test Loss: 0.02097, Test Acc: 37.80%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 270.21it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1517.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 13 | Loss: 1.69382, Acc: 40.72% | Test Loss: 0.02089, Test Acc: 37.57%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 272.09it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1486.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 14 | Loss: 1.69360, Acc: 40.63% | Test Loss: 0.02085, Test Acc: 37.63%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 283.93it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1523.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 15 | Loss: 1.69349, Acc: 40.60% | Test Loss: 0.02085, Test Acc: 37.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 269.28it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1373.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 16 | Loss: 1.69372, Acc: 40.71% | Test Loss: 0.02084, Test Acc: 37.57%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 275.06it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1472.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 17 | Loss: 1.69334, Acc: 40.66% | Test Loss: 0.02074, Test Acc: 37.31%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 271.48it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1640.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 18 | Loss: 1.69347, Acc: 40.61% | Test Loss: 0.02074, Test Acc: 37.43%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 284.77it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1320.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 19 | Loss: 1.69377, Acc: 40.65% | Test Loss: 0.02079, Test Acc: 37.59%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 270.88it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1435.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20 | Loss: 1.69327, Acc: 40.69% | Test Loss: 0.02078, Test Acc: 37.55%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 279.51it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1548.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 21 | Loss: 1.69300, Acc: 40.71% | Test Loss: 0.02077, Test Acc: 37.63%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 280.00it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1419.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 22 | Loss: 1.69297, Acc: 40.67% | Test Loss: 0.02077, Test Acc: 37.59%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 278.66it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1504.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 23 | Loss: 1.69287, Acc: 40.66% | Test Loss: 0.02077, Test Acc: 37.59%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 287.31it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1478.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 24 | Loss: 1.69306, Acc: 40.73% | Test Loss: 0.02077, Test Acc: 37.59%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 269.94it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1538.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 25 | Loss: 1.69331, Acc: 40.68% | Test Loss: 0.02076, Test Acc: 37.57%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 285.22it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1560.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 26 | Loss: 1.69294, Acc: 40.72% | Test Loss: 0.02075, Test Acc: 37.57%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 282.09it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1456.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 27 | Loss: 1.69317, Acc: 40.72% | Test Loss: 0.02076, Test Acc: 37.57%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training... : 100%|| 317/317 [00:01<00:00, 287.38it/s]\n",
            "Validating... : 100%|| 80/80 [00:00<00:00, 1550.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "early stopping happend!\n",
            "Training complete in 0m 35s\n",
            "Best val Acc: 37.823425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we save the model"
      ],
      "metadata": {
        "id": "XA-a_6Vmrvez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_comb_trained.state_dict(), '/content/gdrive/MyDrive/Phase_1_models/comb_trained')"
      ],
      "metadata": {
        "id": "qvscnP_BgBq7"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we test the outputs"
      ],
      "metadata": {
        "id": "RZ61y9yrryM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_comb_trained.eval()\n",
        "acc_test = 0\n",
        "prob_train=torch.tensor([])\n",
        "label_train=torch.tensor([])\n",
        "with torch.inference_mode():\n",
        "  for batch,label in tqdm(train_loader, total=len(train_loader), desc=\"Testing... \"):\n",
        "  # 1. Forward pass\n",
        "    batch = batch.to(device)\n",
        "    label = label.to(device)\n",
        "    output = model_comb_trained(batch) # model outputs raw logits \n",
        "    _, preds = torch.max(output, 1)\n",
        "    prob_train=torch.cat((prob_train,preds))\n",
        "    label_train=torch.cat((label_train,preds))\n",
        "    acc_test += torch.sum(preds == label.data)\n",
        "print(\"\\n\")\n",
        "print(f'accuracy on test data = {acc_test/len(train_dataset)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV5X6LGlKW0j",
        "outputId": "59224529-b0fd-402e-a91f-3045ff6b3068"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing... : 100%|| 317/317 [00:00<00:00, 1555.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "accuracy on test data = 0.4025691747665405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_comb_trained.eval()\n",
        "acc_test = 0\n",
        "prob_val=torch.tensor([])\n",
        "label_val=torch.tensor([])\n",
        "with torch.inference_mode():\n",
        "  for batch,label in tqdm(val_loader, total=len(val_loader), desc=\"Testing... \"):\n",
        "  # 1. Forward pass\n",
        "    batch = batch.to(device)\n",
        "    label = label.to(device)\n",
        "    output = model_comb_trained(batch) # model outputs raw logits \n",
        "    _, preds = torch.max(output, 1)\n",
        "    prob_val=torch.cat((prob_val,preds))\n",
        "    label_val=torch.cat((label_val,preds))\n",
        "    acc_test += torch.sum(preds == label.data)\n",
        "print(\"\\n\")\n",
        "print(f'accuracy on test data = {acc_test/len(val_dataset)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOkXDoSEKf_j",
        "outputId": "267208b7-2dcd-464c-b5ed-2d6563537755"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing... : 100%|| 80/80 [00:00<00:00, 1450.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "accuracy on test data = 0.37823423743247986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_comb_trained.eval()\n",
        "acc_test = 0\n",
        "prob_test=torch.tensor([])\n",
        "label_test=torch.tensor([])\n",
        "with torch.inference_mode():\n",
        "  for batch,label in tqdm(test_loader, total=len(test_loader), desc=\"Testing... \"):\n",
        "  # 1. Forward pass\n",
        "    batch = batch.to(device)\n",
        "    label = label.to(device)\n",
        "    output = model_comb_trained(batch) # model outputs raw logits \n",
        "    _, preds = torch.max(output, 1)\n",
        "    prob_test=torch.cat((prob_test,preds))\n",
        "    label_test=torch.cat((label_test,preds))\n",
        "    acc_test += torch.sum(preds == label.data)\n",
        "print(\"\\n\")\n",
        "print(f'accuracy on test data = {acc_test/len(test_dataset)}')"
      ],
      "metadata": {
        "id": "0GbAHqDXgL5i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d96aff8c-197a-43e8-e640-45f4dfc4f0ac"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing... : 100%|| 80/80 [00:00<00:00, 1613.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "accuracy on test data = 0.4012235999107361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we can see that the combination of the two previous models helped all train, validation and test results improved to a greater amount. now we have a test accuracy of 40%"
      ],
      "metadata": {
        "id": "gySZEnMNr14b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we try to provide confusion matrix for each data"
      ],
      "metadata": {
        "id": "mlDrSv2ssGhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_manual(X,t):\n",
        "  \"\"\"\n",
        "  manual function for implementing softmax\n",
        "  input:\n",
        "    X:input, each coloumn is seperate data\n",
        "    t:temperature value\n",
        "  output:\n",
        "    y\n",
        "  \"\"\"\n",
        "  \n",
        "  _,m = X.size()\n",
        "  m = int(m)\n",
        "  x_max = (torch.max(X,axis = 1)).values\n",
        "  X = (X.T - x_max).T #adding value doesn't have effect on softmax output but it helpd to stablaize our values!(because it is ways easier to comput exp(-145) compare to exp(145)! one is near zero and one is very big number that we can't store in memory)\n",
        "  exp_X = torch.exp(X/t)\n",
        "  exp_X_sum = torch.sum(exp_X,axis = 1)\n",
        "  exp_X_sum_rep = exp_X_sum.repeat(m,1).T\n",
        "  y = exp_X/exp_X_sum_rep\n",
        "  return y"
      ],
      "metadata": {
        "id": "NVNF1ayClodf"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2-qEyIPmLEK",
        "outputId": "ad2c8ad0-df89-4e54-86fd-ac09a63a415e"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_comb_trained.eval()\n",
        "out_logits_train = np.zeros((len(train_dataset),3))\n",
        "out_logits_val = np.zeros((len(val_dataset),3))\n",
        "out_logits_test = np.zeros((len(test_dataset),3))\n",
        "counter = 0\n",
        "for data,_ in train_loader:\n",
        "  data= data.to(device)\n",
        "  out_l = model_comb_trained(data)\n",
        "  out = softmax_manual(out_l,0.1)\n",
        "  out = out.to(device)\n",
        "  out_logits_train[counter:counter +batch_size ] = out.detach().numpy()\n",
        "  counter = counter + batch_size\n",
        "counter = 0\n",
        "for data,_ in val_loader:\n",
        "  data = data.to(device)\n",
        "  out_l = model_comb_trained(data)\n",
        "  out = softmax_manual(out_l,0.1)\n",
        "  out = out.to(device)\n",
        "  out_logits_val[counter:counter +batch_size ] = out.detach().numpy()\n",
        "  counter = counter + batch_size\n",
        "counter = 0\n",
        "for data,_ in test_loader:\n",
        "  data = data.to(device)\n",
        "  out_l = model_comb_trained(data)\n",
        "  out = softmax_manual(out_l,0.1)\n",
        "  out = out.to(device)\n",
        "  out_logits_test[counter:counter +batch_size ] = out.detach().numpy()\n",
        "  counter = counter + batch_size\n"
      ],
      "metadata": {
        "id": "WS8gizvxlsln"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_train = np.argmax(out_logits_train,axis = 1)\n",
        "predict_val = np.argmax(out_logits_val,axis = 1)\n",
        "predict_test = np.argmax(out_logits_test,axis = 1)"
      ],
      "metadata": {
        "id": "EYpsA-sKmosq"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "confusion_matrix = metrics.confusion_matrix(sentiment_train, predict_train)\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['0', '1','2'])\n",
        "cm_display.plot()\n",
        "plt.title('confusion matrix for train data')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "nseKmcwAZ9Wf",
        "outputId": "53c4a38a-a4ff-4f59-c211-093aebf4a28a"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "confusion_matrix = metrics.confusion_matrix(sentiment_dev, predict_val)\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['0', '1','2'])\n",
        "cm_display.plot()\n",
        "plt.title('confusion matrix for validation data')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "L6lICdpyaBG2",
        "outputId": "680477f6-74ba-4110-a33d-83b2a4940adf"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "confusion_matrix = metrics.confusion_matrix(sentiment_test, predict_test)\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = ['0', '1','2'])\n",
        "cm_display.plot()\n",
        "plt.title('confusion matrix for validation data')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "au2dwRXSjG-Z",
        "outputId": "e5abfcf0-a7ed-4cca-af73-a2cb2f5a9335"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "overall the results are better in combined mode than each of the single models."
      ],
      "metadata": {
        "id": "mUriHN_BsPBE"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cbc304ededc24f948a096329873c93ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97e19304a38943b88285c3de64b79f1e",
              "IPY_MODEL_5948eb42e4254cd3baeab93cfe033954",
              "IPY_MODEL_1911737ed9934f6780fe07d35c1105e4"
            ],
            "layout": "IPY_MODEL_d208d5e04ad84bf5bbe73eadd8191341"
          }
        },
        "97e19304a38943b88285c3de64b79f1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26ac259411b94faeb938608902fd47cd",
            "placeholder": "",
            "style": "IPY_MODEL_18d1e32b481b46879788f946a251c271",
            "value": "100%"
          }
        },
        "5948eb42e4254cd3baeab93cfe033954": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f2afa822e954efd90c6f3a7f991937a",
            "max": 553433881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1feeadf5a1b54c409eaa2d95fb64add9",
            "value": 553433881
          }
        },
        "1911737ed9934f6780fe07d35c1105e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a1247bec64a41ddbcd96a6ebe6fc3ef",
            "placeholder": "",
            "style": "IPY_MODEL_ca05efafcac044b78a24cd7e68862f93",
            "value": " 528M/528M [00:02&lt;00:00, 260MB/s]"
          }
        },
        "d208d5e04ad84bf5bbe73eadd8191341": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26ac259411b94faeb938608902fd47cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18d1e32b481b46879788f946a251c271": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f2afa822e954efd90c6f3a7f991937a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1feeadf5a1b54c409eaa2d95fb64add9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a1247bec64a41ddbcd96a6ebe6fc3ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca05efafcac044b78a24cd7e68862f93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad6b896fe1b24d98bc8e1c7531e46882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_849ebfe1e5844bf88d5e35123296126d",
              "IPY_MODEL_885a6216816341f2b8d201161e526652",
              "IPY_MODEL_985aca2907484a4faa6f9755b2a91d2a"
            ],
            "layout": "IPY_MODEL_5bab55662b0e48d38652522e0fbdbead"
          }
        },
        "849ebfe1e5844bf88d5e35123296126d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fb15503dadd4dbfb9f6c20b2763f5d6",
            "placeholder": "",
            "style": "IPY_MODEL_d19ac633783641e98dee566bb32f2608",
            "value": "100%"
          }
        },
        "885a6216816341f2b8d201161e526652": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d437e00e204f4bb188bf61284008de56",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad5fefe0ff674b328f6fbbee0b9389dc",
            "value": 102530333
          }
        },
        "985aca2907484a4faa6f9755b2a91d2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c03eaa59f7f4961a69980ec05c55787",
            "placeholder": "",
            "style": "IPY_MODEL_9e31e3432d4545188cc2c5723c8107c6",
            "value": " 97.8M/97.8M [00:01&lt;00:00, 94.6MB/s]"
          }
        },
        "5bab55662b0e48d38652522e0fbdbead": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fb15503dadd4dbfb9f6c20b2763f5d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d19ac633783641e98dee566bb32f2608": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d437e00e204f4bb188bf61284008de56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad5fefe0ff674b328f6fbbee0b9389dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c03eaa59f7f4961a69980ec05c55787": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e31e3432d4545188cc2c5723c8107c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c304e8f066e0465891f956622e3106c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44e96b33dc1e491f9d23cc97daf193cc",
              "IPY_MODEL_77015872652d45288d002e5d31712346",
              "IPY_MODEL_6b7633308bdd421280f1897c57e8ff63"
            ],
            "layout": "IPY_MODEL_33a2ea36dfe34b6b88070a121f49f123"
          }
        },
        "44e96b33dc1e491f9d23cc97daf193cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_908b4374738944a2822d3c6cd24b2532",
            "placeholder": "",
            "style": "IPY_MODEL_8e961932ecba41aa8124f57ae6849999",
            "value": "100%"
          }
        },
        "77015872652d45288d002e5d31712346": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c4bf9796c3f418fb93ff1e221ddf0af",
            "max": 1789735,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4151b27d53a40ce9c1693c9e85ef2c3",
            "value": 1789735
          }
        },
        "6b7633308bdd421280f1897c57e8ff63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f131bf76cef4c32932939ac86ede070",
            "placeholder": "",
            "style": "IPY_MODEL_396b2ce674d04da79041168be66461f5",
            "value": " 1.71M/1.71M [00:00&lt;00:00, 5.51MB/s]"
          }
        },
        "33a2ea36dfe34b6b88070a121f49f123": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "908b4374738944a2822d3c6cd24b2532": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e961932ecba41aa8124f57ae6849999": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c4bf9796c3f418fb93ff1e221ddf0af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4151b27d53a40ce9c1693c9e85ef2c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f131bf76cef4c32932939ac86ede070": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "396b2ce674d04da79041168be66461f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}